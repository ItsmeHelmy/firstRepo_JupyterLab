{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe6e644-af88-4e8a-aca2-8a79ae739670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      " 12  955k   12  119k    0     0  56252      0  0:00:17  0:00:02  0:00:15 56300\n",
      "100  955k  100  955k    0     0   332k      0  0:00:02  0:00:02 --:--:--  333k\n"
     ]
    }
   ],
   "source": [
    "# Data info\n",
    "# F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets\n",
    "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dbb97-5629-4c46-824e-12b105a17e41",
   "metadata": {},
   "source": [
    "Recommendation System: Collaborative System utilize Matrix Factorization and Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c6a80-8d51-4185-b5ff-60417d8b3d60",
   "metadata": {},
   "source": [
    "Start working with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341f5be0-facd-4e96-83fb-067575fe1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288dd218-c7d9-41de-b3fe-f672014e977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
    "ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a231df95-94dc-4cd8-88ca-fa4bb1818177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of movies dataframe are:  (9742, 3) \n",
      "The dimensions of ratings dataframe are: (100836, 4)\n"
     ]
    }
   ],
   "source": [
    "print('The dimension of movies dataframe are: ', movies_df.shape, '\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57933600-cab3-4df5-b9a3-3a7390279137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
       "1                   Adventure|Children|Fantasy  \n",
       "2                               Comedy|Romance  \n",
       "3                         Comedy|Drama|Romance  \n",
       "4                                       Comedy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b383a404-521f-4d0a-89b2-125070ea0943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b14809b-e2fc-49c2-907b-47dc4d50249f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uniqe users: 610\n",
      "Number of uniqe users: 9724\n",
      "The full rating matrix will have: 5931640 elements. \n",
      "-------------------------------------------------\n",
      "Number of ratings: 100836\n",
      "This means:  1.6999683055613624 % of the matrix is filled.\n"
     ]
    }
   ],
   "source": [
    "# Movie ID to movie name mapping\n",
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "print(\"Number of uniqe users:\", n_users)\n",
    "print(\"Number of uniqe users:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements. ')\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Number of ratings:\", len(ratings_df))\n",
    "print(\"This means: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19e928f-829f-4ca1-83a9-a9f135b339fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a loop table for the input.\n",
    "        # create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a loop table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc6e521-5988-4393-a894-8b11279fc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataloader (for pytorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader #this helps transfrom your data to ML readiness\n",
    "\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "\n",
    "        # Extract all user IDs and movie IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "\n",
    "        # Producing new countinous IDs for users and movies\n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "\n",
    "        # Obtained continous ID for users and movies\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "\n",
    "        # Return the id from the indexed values as noted in the lambda function\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "\n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensord (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1e5c52-e6d1-4647-a52e-ca9d6a185e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "Is running on GPU:  True\n",
      "MatrixFactorization(\n",
      "  (user_factors): Embedding(610, 8)\n",
      "  (item_factors): Embedding(9724, 8)\n",
      ")\n",
      "user_factors.weight tensor([[0.0393, 0.0485, 0.0473,  ..., 0.0099, 0.0455, 0.0435],\n",
      "        [0.0445, 0.0073, 0.0313,  ..., 0.0192, 0.0036, 0.0277],\n",
      "        [0.0249, 0.0258, 0.0230,  ..., 0.0009, 0.0261, 0.0283],\n",
      "        ...,\n",
      "        [0.0045, 0.0489, 0.0490,  ..., 0.0167, 0.0297, 0.0319],\n",
      "        [0.0267, 0.0430, 0.0371,  ..., 0.0056, 0.0223, 0.0064],\n",
      "        [0.0012, 0.0453, 0.0346,  ..., 0.0434, 0.0061, 0.0011]])\n",
      "item_factors.weight tensor([[0.0181, 0.0393, 0.0018,  ..., 0.0355, 0.0004, 0.0389],\n",
      "        [0.0352, 0.0416, 0.0482,  ..., 0.0490, 0.0323, 0.0259],\n",
      "        [0.0065, 0.0159, 0.0253,  ..., 0.0342, 0.0054, 0.0243],\n",
      "        ...,\n",
      "        [0.0303, 0.0052, 0.0035,  ..., 0.0262, 0.0382, 0.0200],\n",
      "        [0.0167, 0.0147, 0.0276,  ..., 0.0117, 0.0002, 0.0248],\n",
      "        [0.0233, 0.0083, 0.0081,  ..., 0.0133, 0.0480, 0.0131]])\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "import torch\n",
    "print(torch.version.cuda)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU: \", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if available\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Adam Optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train model\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd8eb3f-adfc-482f-9a7c-8cce9da7db07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711f5327dbe14e11af8dd42b969a06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter #0 Loss: 11.0675803997795\n",
      "iter #1 Loss: 4.74774285349144\n",
      "iter #2 Loss: 2.4771450882030623\n",
      "iter #3 Loss: 1.722094149002569\n",
      "iter #4 Loss: 1.346322514305865\n",
      "iter #5 Loss: 1.1290845287027698\n",
      "iter #6 Loss: 0.9917883684641213\n",
      "iter #7 Loss: 0.9007227372397021\n",
      "iter #8 Loss: 0.8372564085092641\n",
      "iter #9 Loss: 0.7924526236232767\n",
      "iter #10 Loss: 0.7595862663412457\n",
      "iter #11 Loss: 0.7348819890527556\n",
      "iter #12 Loss: 0.7161125317594121\n",
      "iter #13 Loss: 0.701716947184904\n",
      "iter #14 Loss: 0.69064312242917\n",
      "iter #15 Loss: 0.68174049054003\n",
      "iter #16 Loss: 0.6751544304608088\n",
      "iter #17 Loss: 0.6697232748575622\n",
      "iter #18 Loss: 0.6658261372958343\n",
      "iter #19 Loss: 0.6631252764899114\n",
      "iter #20 Loss: 0.6605720669031143\n",
      "iter #21 Loss: 0.6589147372899322\n",
      "iter #22 Loss: 0.6579325071673103\n",
      "iter #23 Loss: 0.656866823772186\n",
      "iter #24 Loss: 0.6560199491414928\n",
      "iter #25 Loss: 0.655037562518858\n",
      "iter #26 Loss: 0.6546409914865712\n",
      "iter #27 Loss: 0.6536765035033831\n",
      "iter #28 Loss: 0.6526153637899965\n",
      "iter #29 Loss: 0.6519604122199988\n",
      "iter #30 Loss: 0.650689675420674\n",
      "iter #31 Loss: 0.6491421987184414\n",
      "iter #32 Loss: 0.6472720886925756\n",
      "iter #33 Loss: 0.645142786053534\n",
      "iter #34 Loss: 0.6422488589577263\n",
      "iter #35 Loss: 0.6388617498317951\n",
      "iter #36 Loss: 0.6344783534555871\n",
      "iter #37 Loss: 0.6293819840184323\n",
      "iter #38 Loss: 0.6239956584451768\n",
      "iter #39 Loss: 0.6167278099181083\n",
      "iter #40 Loss: 0.6090780164535881\n",
      "iter #41 Loss: 0.6007808757388047\n",
      "iter #42 Loss: 0.5914550154720466\n",
      "iter #43 Loss: 0.581628673799752\n",
      "iter #44 Loss: 0.5715220642422661\n",
      "iter #45 Loss: 0.561498903229757\n",
      "iter #46 Loss: 0.5512620930366104\n",
      "iter #47 Loss: 0.5417279124184309\n",
      "iter #48 Loss: 0.5316620932012646\n",
      "iter #49 Loss: 0.5223245253493338\n",
      "iter #50 Loss: 0.5130274941776004\n",
      "iter #51 Loss: 0.5046798029753763\n",
      "iter #52 Loss: 0.4962701727896172\n",
      "iter #53 Loss: 0.48845851324914674\n",
      "iter #54 Loss: 0.48045875782591435\n",
      "iter #55 Loss: 0.47347038689301096\n",
      "iter #56 Loss: 0.4664605342721576\n",
      "iter #57 Loss: 0.46000685781089184\n",
      "iter #58 Loss: 0.4540800408928225\n",
      "iter #59 Loss: 0.44815164660741835\n",
      "iter #60 Loss: 0.4428028523316843\n",
      "iter #61 Loss: 0.4372977465391159\n",
      "iter #62 Loss: 0.43274127620125785\n",
      "iter #63 Loss: 0.4277560351993227\n",
      "iter #64 Loss: 0.42311100162527887\n",
      "iter #65 Loss: 0.4190465677435023\n",
      "iter #66 Loss: 0.41507228021391757\n",
      "iter #67 Loss: 0.4110811470161537\n",
      "iter #68 Loss: 0.40733214123600026\n",
      "iter #69 Loss: 0.40409165465740987\n",
      "iter #70 Loss: 0.400738332496228\n",
      "iter #71 Loss: 0.39763081805581973\n",
      "iter #72 Loss: 0.3944854051215092\n",
      "iter #73 Loss: 0.39182555077947334\n",
      "iter #74 Loss: 0.3890474572929029\n",
      "iter #75 Loss: 0.3863153616486467\n",
      "iter #76 Loss: 0.3838132679991916\n",
      "iter #77 Loss: 0.38144303504661253\n",
      "iter #78 Loss: 0.37903506027033484\n",
      "iter #79 Loss: 0.3767196781328184\n",
      "iter #80 Loss: 0.3746894291852634\n",
      "iter #81 Loss: 0.37270768858530195\n",
      "iter #82 Loss: 0.37069003179971943\n",
      "iter #83 Loss: 0.3688759692958769\n",
      "iter #84 Loss: 0.36709418109982145\n",
      "iter #85 Loss: 0.3652733391538489\n",
      "iter #86 Loss: 0.3638722257414445\n",
      "iter #87 Loss: 0.36201856209693223\n",
      "iter #88 Loss: 0.3607958569404135\n",
      "iter #89 Loss: 0.3589707810412809\n",
      "iter #90 Loss: 0.3577600452803113\n",
      "iter #91 Loss: 0.3563671694302619\n",
      "iter #92 Loss: 0.3552051413671922\n",
      "iter #93 Loss: 0.35382914151712724\n",
      "iter #94 Loss: 0.35246062447047477\n",
      "iter #95 Loss: 0.3514699418910869\n",
      "iter #96 Loss: 0.3501877745585091\n",
      "iter #97 Loss: 0.3490215608765026\n",
      "iter #98 Loss: 0.3481308648428941\n",
      "iter #99 Loss: 0.3469934276669159\n",
      "iter #100 Loss: 0.3460619918018731\n",
      "iter #101 Loss: 0.3450477021335043\n",
      "iter #102 Loss: 0.34410322813061894\n",
      "iter #103 Loss: 0.34322020933410236\n",
      "iter #104 Loss: 0.3424330076514767\n",
      "iter #105 Loss: 0.3413833361780885\n",
      "iter #106 Loss: 0.34069771590042236\n",
      "iter #107 Loss: 0.33991552951750414\n",
      "iter #108 Loss: 0.3391838430753214\n",
      "iter #109 Loss: 0.33839017670923077\n",
      "iter #110 Loss: 0.33770359470154426\n",
      "iter #111 Loss: 0.33689346209849197\n",
      "iter #112 Loss: 0.3361146166012977\n",
      "iter #113 Loss: 0.33571537862044903\n",
      "iter #114 Loss: 0.33488425860171994\n",
      "iter #115 Loss: 0.33441979196017163\n",
      "iter #116 Loss: 0.3336493594543583\n",
      "iter #117 Loss: 0.3330027597393784\n",
      "iter #118 Loss: 0.3324662777902511\n",
      "iter #119 Loss: 0.3318980734057838\n",
      "iter #120 Loss: 0.33130343228124726\n",
      "iter #121 Loss: 0.3307006574297314\n",
      "iter #122 Loss: 0.3303223884839394\n",
      "iter #123 Loss: 0.3295241334111557\n",
      "iter #124 Loss: 0.3290492681606772\n",
      "iter #125 Loss: 0.32876978617936825\n",
      "iter #126 Loss: 0.3281965541658063\n",
      "iter #127 Loss: 0.3275388435928652\n"
     ]
    }
   ],
   "source": [
    "for it in tqdm(range(num_epochs)):\n",
    "    losses = []\n",
    "    for x,y in train_loader:\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4486c337-9d86-4281-851d-74d7ee0d4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[ 1.3404,  1.1545,  0.9744,  ...,  0.9171,  1.4148,  1.4991],\n",
      "        [-0.3588,  0.8034,  1.2650,  ...,  1.9001,  1.3248,  0.6369],\n",
      "        [-0.7991, -0.3714,  0.9428,  ...,  3.0603,  1.9288,  0.9640],\n",
      "        ...,\n",
      "        [-0.3642,  1.1918,  2.9811,  ...,  1.0466,  0.4813,  1.1835],\n",
      "        [ 0.8912,  0.4304,  0.8548,  ...,  1.2422,  1.2165,  1.0027],\n",
      "        [ 0.8427,  2.0820,  1.2852,  ...,  1.3544,  1.2635,  0.4195]],\n",
      "       device='cuda:0')\n",
      "item_factors.weight tensor([[ 0.4935,  0.9306,  0.1590,  ...,  0.3412,  0.3901,  0.4233],\n",
      "        [ 0.6151,  0.4735,  0.2595,  ...,  0.7328,  0.1241, -0.1935],\n",
      "        [ 0.3260,  0.7434,  0.6546,  ...,  0.5516,  0.2806,  0.4545],\n",
      "        ...,\n",
      "        [ 0.3696,  0.3486,  0.3474,  ...,  0.3690,  0.3798,  0.3527],\n",
      "        [ 0.4124,  0.4114,  0.4240,  ...,  0.4078,  0.3970,  0.4028],\n",
      "        [ 0.4033,  0.3871,  0.3869,  ...,  0.3908,  0.4260,  0.3680]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# By training model , we will have turned latent factors for movies and users\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "            uw = param.data\n",
    "            c += 1\n",
    "        else:\n",
    "            iw = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7de078b1-8a2a-4302-8077-f3a07e282957",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad651292-2645-4aba-b77f-727f60efca2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49352616,  0.9306135 ,  0.15904601, ...,  0.34116322,\n",
       "         0.39008784,  0.42332396],\n",
       "       [ 0.615147  ,  0.47346   ,  0.25952616, ...,  0.7327716 ,\n",
       "         0.12405194, -0.1935458 ],\n",
       "       [ 0.3260084 ,  0.7434126 ,  0.6545562 , ...,  0.5515972 ,\n",
       "         0.2805826 ,  0.45453426],\n",
       "       ...,\n",
       "       [ 0.3696093 ,  0.34859645,  0.34743902, ...,  0.36900786,\n",
       "         0.37982777,  0.35272756],\n",
       "       [ 0.4124302 ,  0.41137502,  0.42395863, ...,  0.4077892 ,\n",
       "         0.39695445,  0.40281463],\n",
       "       [ 0.4033077 ,  0.38707095,  0.3868785 , ...,  0.3907948 ,\n",
       "         0.426049  ,  0.3679608 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_movie_embeddings # unique movie factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed7e2c6f-7c33-4c26-9ac5-70f033f4327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the cluster based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "152f6512-a147-4389-9311-437932495284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "'''\n",
    "It can be seen here that the movies which are in the same cluster tend to have similar genres. \n",
    "Also note that the algorithm in unfamilliar with the movie name and only obtained the relationship\n",
    "by looking at the numbers representing how users have responded to the movie selections.\n",
    "'''\n",
    "def display_clusters(kmeans, train_set, movie_names, ratings_df):\n",
    "    for cluster in range(10):\n",
    "        print(\"Cluster #{}\".format(cluster))\n",
    "        movs = []\n",
    "        for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "            movid = train_set.idx2movieid[movidx]\n",
    "            rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[0]\n",
    "            movs.append((movie_names[movid], rat_count))\n",
    "        for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "            print(\"\\t\", mov[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a16cea8-9aca-4c04-9095-f8c472136efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Mask, The (1994)\n",
      "\t Cliffhanger (1993)\n",
      "\t Mummy, The (1999)\n",
      "\t Mars Attacks! (1996)\n",
      "\t Santa Clause, The (1994)\n",
      "\t Demolition Man (1993)\n",
      "\t Starship Troopers (1997)\n",
      "\t Face/Off (1997)\n",
      "\t Desperado (1995)\n",
      "\t Blair Witch Project, The (1999)\n",
      "Cluster #1\n",
      "\t Forrest Gump (1994)\n",
      "\t Shawshank Redemption, The (1994)\n",
      "\t Silence of the Lambs, The (1991)\n",
      "\t Matrix, The (1999)\n",
      "\t Jurassic Park (1993)\n",
      "\t Braveheart (1995)\n",
      "\t Terminator 2: Judgment Day (1991)\n",
      "\t Schindler's List (1993)\n",
      "\t Star Wars: Episode V - The Empire Strikes Back (1980)\n",
      "\t Seven (a.k.a. Se7en) (1995)\n",
      "Cluster #2\n",
      "\t Honey, I Shrunk the Kids (1989)\n",
      "\t Sabrina (1995)\n",
      "\t Last Action Hero (1993)\n",
      "\t Superman II (1980)\n",
      "\t Nine Months (1995)\n",
      "\t Batman & Robin (1997)\n",
      "\t Hellboy (2004)\n",
      "\t Junior (1994)\n",
      "\t Godzilla (1998)\n",
      "\t Toys (1992)\n",
      "Cluster #3\n",
      "\t Ace Ventura: Pet Detective (1994)\n",
      "\t Austin Powers: The Spy Who Shagged Me (1999)\n",
      "\t Bourne Identity, The (2002)\n",
      "\t Big Lebowski, The (1998)\n",
      "\t WALLÂ·E (2008)\n",
      "\t Austin Powers: International Man of Mystery (1997)\n",
      "\t Happy Gilmore (1996)\n",
      "\t Total Recall (1990)\n",
      "\t Ace Ventura: When Nature Calls (1995)\n",
      "\t Blues Brothers, The (1980)\n",
      "Cluster #4\n",
      "\t Dumb & Dumber (Dumb and Dumber) (1994)\n",
      "\t Clueless (1995)\n",
      "\t Broken Arrow (1996)\n",
      "\t Nutty Professor, The (1996)\n",
      "\t Pirates of the Caribbean: Dead Man's Chest (2006)\n",
      "\t Legends of the Fall (1994)\n",
      "\t Coneheads (1993)\n",
      "\t Signs (2002)\n",
      "\t I Am Legend (2007)\n",
      "\t Three Musketeers, The (1993)\n",
      "Cluster #5\n",
      "\t Batman Forever (1995)\n",
      "\t GoldenEye (1995)\n",
      "\t Iron Man (2008)\n",
      "\t Addams Family Values (1993)\n",
      "\t Hangover, The (2009)\n",
      "\t Avengers, The (2012)\n",
      "\t Pocahontas (1995)\n",
      "\t Grease (1978)\n",
      "\t Juno (2007)\n",
      "\t Casper (1995)\n",
      "Cluster #6\n",
      "\t Pulp Fiction (1994)\n",
      "\t Fight Club (1999)\n",
      "\t Usual Suspects, The (1995)\n",
      "\t American Beauty (1999)\n",
      "\t Godfather, The (1972)\n",
      "\t Fargo (1996)\n",
      "\t Memento (2000)\n",
      "\t Monty Python and the Holy Grail (1975)\n",
      "\t One Flew Over the Cuckoo's Nest (1975)\n",
      "\t Reservoir Dogs (1992)\n",
      "Cluster #7\n",
      "\t Waterworld (1995)\n",
      "\t Net, The (1995)\n",
      "\t Natural Born Killers (1994)\n",
      "\t Casino (1995)\n",
      "\t Mr. Holland's Opus (1995)\n",
      "\t Erin Brockovich (2000)\n",
      "\t Scream (1996)\n",
      "\t Bridget Jones's Diary (2001)\n",
      "\t Gone in 60 Seconds (2000)\n",
      "\t Congo (1995)\n",
      "Cluster #8\n",
      "\t Independence Day (a.k.a. ID4) (1996)\n",
      "\t True Lies (1994)\n",
      "\t Dances with Wolves (1990)\n",
      "\t Stargate (1994)\n",
      "\t Star Wars: Episode I - The Phantom Menace (1999)\n",
      "\t Titanic (1997)\n",
      "\t Pretty Woman (1990)\n",
      "\t X-Men (2000)\n",
      "\t Babe (1995)\n",
      "\t Twister (1996)\n",
      "Cluster #9\n",
      "\t Star Wars: Episode IV - A New Hope (1977)\n",
      "\t Toy Story (1995)\n",
      "\t Lord of the Rings: The Fellowship of the Ring, The (2001)\n",
      "\t Fugitive, The (1993)\n",
      "\t Lord of the Rings: The Two Towers, The (2002)\n",
      "\t Aladdin (1992)\n",
      "\t Beauty and the Beast (1991)\n",
      "\t Mrs. Doubtfire (1993)\n",
      "\t Groundhog Day (1993)\n",
      "\t Terminator, The (1984)\n"
     ]
    }
   ],
   "source": [
    "display_clusters(kmeans, train_set, movie_names, ratings_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
